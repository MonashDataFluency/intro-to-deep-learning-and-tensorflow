{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"Introduction to Convolutional Neural Networks.ipynb","provenance":[{"file_id":"1MUd5js2JoUjp27XVuitKGF2XhRcvXbWn","timestamp":1586410795392}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-0yJIfw7h30J","colab_type":"text"},"source":["# Introduction to Convolutional Neural Networks\n","\n","## Welcome!\n","This exercise serves as an introduction to building, training and testing a convolutional neural network using TensorFlow.\n","\n","At the end of this exercise, you should be confident to use TensorFlow to train your own conv-net on image data (or spatially related data) of your choice and to analyse and interprete the output of the model.\n","\n","You may refer to the lecture slides that accompany this notebook, in case you need a reference point.\n","\n","## Preparing this notebook\n","Let us start by setting up our coding environment. \n","\n","The first thing you might want to do is to enable GPU usage for this notebook. You can do so by going to Edit -> Notebook Settings, clicking on Hardware Accelerator and selecting GPU. This is not strictly necessary for this exercise as we are working with small datasets and small networks. However, it does help speed up computation even with what we are working with.\n","\n","Next, lets import all the software packages that we will need for this exercise, most importantly TensorFlow. You may do so by running the block of code below by clicking the right-arrow button at the top left of the cell. If an error occurs, the circle would turn red - get help from your instructor. If it runs fine, you should see the currently used version of TensorFlow printed out below."]},{"cell_type":"code","metadata":{"id":"jjSdcVkKh30K","colab_type":"code","colab":{}},"source":["# Load all libraries and helper functions\n","import tensorflow as tf\n","import numpy as np\n","import sys\n","import pandas as pd\n","import os\n","import urllib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","print(\"TensorFlow version %s is loaded.\" % tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7Przbq8h30N","colab_type":"text"},"source":["## Sanity check\n","Let's check that TensorFlow works correctly by creating a tensor that contains a constant string \"Hello World!\" and then printing it out to console."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MKlLEYyAyKTa","colab":{}},"source":["hello_world = tf.constant(\"Hello World!\", name=\"sanity_check\")\n","print('Prints the tensor itself: ', hello_world)\n","tf.print('Prints the values of the tensor: ', hello_world, output_stream=sys.stdout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P2ERlv06yWSR"},"source":["Notice that the variable hello_world itself is a tensor. If you wish to print the values of a tensor, you could use the tf.print method."]},{"cell_type":"markdown","metadata":{"id":"oBUQLREJh30u","colab_type":"text"},"source":["## Inspecting the data\n","\n","We will be working with the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a very popular dataset of hand-written digits.\n","\n","The dataset contains 70,000 images, where each image contains a single hand-written digit from 0 to 9 and is of size 28x28 pixels in greyscale. Each image also has an accompanying label: a digit (0 to 9) representing the digit that the image contains. Our aim is to build a convolutional neural network that takes as input each image, and predicts (as a single digit) what digit the image contains.\n","\n","The code below loads and scales the data for our use. The data is automatically grouped into inputs and labels, and split into a training and testing set.\n"]},{"cell_type":"code","metadata":{"id":"HWLLWFVT4_jw","colab_type":"code","colab":{}},"source":["# Load the mnist dataset built into TensorFlow\n","mnist = tf.keras.datasets.mnist\n","(inputs_train, labels_train), (inputs_test, labels_test) = mnist.load_data()\n","# Squash the training data to values that range between 0 and 1.\n","inputs_train, inputs_test = inputs_train / 255.0, inputs_test / 255.0\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7o87-bdh50y4","colab_type":"text"},"source":["Let's try to visualise some of the images. Run the following code a few times to inspect multiple samples."]},{"cell_type":"code","metadata":{"id":"c5Kvg0yt552F","colab_type":"code","colab":{}},"source":["plt.figure(1, figsize=(20,10))\n","for i in range(5):\n","    char_idx = np.random.randint(inputs_train.shape[0])\n","    plt.subplot(150 + i + 1)\n","    plt.title(\"Label is the digit %s\" % (labels_train[char_idx]))\n","    plt.imshow(inputs_train[char_idx], cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RCdDljbX65-B","colab_type":"text"},"source":["Print the shape of each of your data objects to have a better idea of how the data is structured.\n","\n","The TensorFlow network that we want to build will take inputs with shape (number of samples, height, width, channels). Does the shapes you see match with the required shape? If not, you may wish to use the [np.expand_dims](https://numpy.org/doc/1.18/reference/generated/numpy.expand_dims.html) function to modify the shape of your data."]},{"cell_type":"code","metadata":{"id":"djrRULjr6Rtm","colab_type":"code","colab":{}},"source":["# Print the shapes of your data objects\n","\n","# Modify the shapes of the relevant data objects\n","# Hint: channels should be 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-OXy4bs8FUW","colab_type":"text"},"source":["Once you have gotten your data into the right shape, we can now use the tf.data API to easily shuffle and batch our dataset. Inspect and run the code cell below.\n","\n","What batch size are we using?"]},{"cell_type":"code","metadata":{"id":"3luGp8bJ8FrP","colab_type":"code","colab":{}},"source":["# Use tf.data to batch and shuffle the dataset:\n","train_ds = tf.data.Dataset.from_tensor_slices((inputs_train, labels_train)).shuffle(10000).batch(32)\n","test_ds = tf.data.Dataset.from_tensor_slices((inputs_test, labels_test)).batch(32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BYuvCk0h30x","colab_type":"text"},"source":["## Building a convolutional neural network\n","Now that we have our data ready, we can switch our attention to designing and constructing our conv-net model.\n","\n","As mentioned above, our aim is to build a convolutional neural network that takes as input each image, and predicts what digit the image contains. Since there are 10 possible digits, we can see this as a classification task across 10 categories.\n","\n","There are generally speaking two ways you can build and train a model in TensorFlow. The first is to use the Keras API such as:\n","*   models.Sequential()\n","*   model.fit()\n","*   model.evaluate()\n","\n","You might already be familiar with this approach. The high level Keras API is very easy to use, however it may lack the flexibility that an advanced user might require. Today we will be looking at the second more manual but also more flexible approach.\n","\n","The code below is an incomplete example showing how a model could be set up using this second approach. Scrutinise the code and add more layers as you wish to complete the network. You may refer to the lecture slides linked above as a guide on how to stack layers to form a conv-net.\n","\n","Some layers you would need are:\n","*   [tf.keras.layers.Conv2D()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) : 2D convolutions.\n","*   [tf.keras.layers.Flatten()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) : To convert the 2D structure of image data into a 1D structure suitable for classification.\n","*   [tf.keras.layers.Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) : Fully connected layers to perform the final classification.\n","\n","Other layers you might want to use are activation function that you can include as part of your Conv2D layers, and Pooling layers. Don't forget to check if each layer would require certain input parameters.\n","\n","Hint: one useful method for debugging the structure of your network is to print out the shape of each layer.\n","\n"]},{"cell_type":"code","metadata":{"id":"lUnsFveb-Y2y","colab_type":"code","colab":{}},"source":["class MyConvNet(tf.keras.Model):\n","  def __init__(self):\n","    super(MyConvNet, self).__init__()\n","    # This function runs whenever you create an instance of your model.\n","    # Since it only runs once, you should initialise all your trainable variables here. Basically any layer that contains \"weights\" in your model.\n","    \n","    # Example of how you would initialise a single layer\n","    self.conv1 = tf.keras.layers.Conv2D(4, 3, activation='relu')\n","    # self.layer2 = ...\n","    # self.layer3 = ...\n","\n","\n","  def call(self, x):\n","    # This function runs every time you call the model: output = model(input).\n","    # The first instance of x above is just a placeholder for inputs to the model.\n","    # Since it runs frequently, you should NOT initialise any trainable variables here.\n","    # If you do so, those weights will be re-initialised at every call, hence defeating the purpose of the training process.\n","\n","    # Example of how you would utilise a single layer initialised above.\n","    x = self.conv1(x)\n","    # x = self.layer2(x)\n","    # x = self.layer3(x)\n","\n","    return x\n","\n","# Create an instance of the model\n","# This instantiation invokes the __init__ function of the model from above.\n","# It then waits for you to call the model to execute the \"call\" function that passes data through your model.\n","model = MyConvNet() \n","\n","# We use SparseCategoricalCrossentropy as the loss function and Adam as the gradient descent function.\n","loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bun4bq5FEHBn","colab_type":"text"},"source":["## Training and testing the model\n","\n","We have specified all components of our model. We can now proceed to training and testing this model.\n","\n","The two functions below define what happens during each training and testing step. Inspect the code as you may need to modify it later. \n","\n","Note that we now use the [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) method to keep track of which operations and variables are necessary in the gradient descent step. This then allows us to perform automatic gradient descent with just a few lines of code."]},{"cell_type":"code","metadata":{"id":"6ySRJKCJEkb4","colab_type":"code","colab":{}},"source":["# We create the x_loss and x_accuracy objects to help keep track of model performance during training. \n","# The loss and accuracy values at each step of the training process are aggregated in the objects and can be printed out at the end of each training epoch.\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","\n","# The function train_step executes the forward and backward propagation of a single batch of images during the training process.\n","# As the TensorFlow documentation puts it, when you annotate a function with tf.function, you can still call it like any other function. \n","# But it will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.\n","@tf.function\n","def train_step(images, labels):\n","  with tf.GradientTape() as tape:\n","    # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n","    # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n","    predictions = model(images, training=True)\n","    train_step_loss = loss_function(labels, predictions)\n","  # Determine the gradients for each trainable variable (weights) based on the loss function\n","  gradients = tape.gradient(train_step_loss, model.trainable_variables)\n","  # Apply the optimiser to the gradients to perform gradient descent\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","  train_loss(train_step_loss)\n","  train_accuracy(labels, predictions)\n","\n","# The function test_step executes the inference (forward propagation) of a single batch of testing image.\n","def test_step(images, labels):\n","  # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n","  # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n","  predictions = model(images, training=False)\n","  test_step_loss = loss_function(labels, predictions)\n","  test_loss(test_step_loss)\n","  test_accuracy(labels, predictions)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHQBplxEHBYQ","colab_type":"text"},"source":["Finally, lets train and test the model with our training script below that calls train_step and test_step from above."]},{"cell_type":"code","metadata":{"id":"BhFmJpdiHQFh","colab_type":"code","colab":{}},"source":["max_epochs = 5\n","for epoch in range(max_epochs):\n","  # Reset the metrics at the start of the next epoch\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_loss.reset_states()\n","  test_accuracy.reset_states()\n","\n","  # Perform training across the entire train set\n","  for inputs, labels in train_ds:\n","    train_step(inputs, labels)\n","\n","  # Perform testing across the entire test set\n","  for test_inputs, test_labels in test_ds:\n","    test_step(test_inputs, test_labels, confusion_matrix)\n","\n","  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n","  print(template.format(epoch+1,\n","                        train_loss.result(),\n","                        train_accuracy.result()*100,\n","                        test_loss.result(),\n","                        test_accuracy.result()*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLRD20yDHfcg","colab_type":"text"},"source":["If you have reached this point, congratulations! You have trained a convolutional neural network to perform classification on image data.\n","\n","Have a look at your training results above. How else can we improve the network, or improve the way we analyse the results?\n","  \n","1.   Add or remove layers from your network. How does it affect model accuracy? You should aim for a test accuracy of >98%.\n","\n","2.   Change the parameters in each layer. Try some extreme values such as very small or large numbers of neurons or convolutional kernels. What changes do you observe?\n","\n","3.   A [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is a good way of visualising classification results. Modify the test_step function above to record the model's predictions in a Confusion matrix. Then print the final matrix together with the accuracy results at the end of each epoch.\n","Hints:\n","   *   You could initialise your confusion matrix as a numpy array: \n","   \n","             confusion_matrix = np.zeros((10,10))\n","   *   You may wish to convert the output of your model into a numpy array before further manipulation. You may also assume that the class which has the largest value is the predicted class: \n","   \n","             pred_class = np.argmax(predictions.numpy(), axis=-1)\n","\n","4.   Based on the confusion matrix from above, can you identify weak points in your model? How would you further improve the model or the dataset given these new observations?\n","\n","5.   We have so far trained the model exclusively on images that contain white handwriting against a black background. What happens when we test the model against inverted images (black handwritting, white background)? You may invert your test images by doing: \n","\n","         inputs_test = 1 - inputs_test\n","\n","6.   Given your observations from inverting your test images, what would you recommend to do to be able to train a model that is more robust to variations in input data?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3younfqTzwk8","colab_type":"text"},"source":["## Answers for reference \n","This section contains the complete code for the exercise in case you are stuck."]},{"cell_type":"code","metadata":{"id":"vf4Oa_RSnj6p","colab_type":"code","colab":{}},"source":["# Load all libraries and helper functions\n","import tensorflow as tf\n","import numpy as np\n","import sys\n","import pandas as pd\n","import os\n","import urllib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","print(\"TensorFlow version %s is loaded.\" % tf.__version__)\n","\n","# Load the mnist dataset built into TensorFlow\n","mnist = tf.keras.datasets.mnist\n","(inputs_train, labels_train), (inputs_test, labels_test) = mnist.load_data()\n","# Squash the training data to values that range between 0 and 1.\n","inputs_train, inputs_test = inputs_train / 255.0, inputs_test / 255.0\n","\n","# TensorFlow networks take inputs with shape (number of samples, height, width, channels)\n","# The current shape for the inputs are (number of samples, height, width)\n","# We therefore need to add an extra dimension at the end\n","inputs_train = np.expand_dims(inputs_train, axis=-1)\n","inputs_test = np.expand_dims(inputs_test, axis=-1)\n","\n","# Print shapes of all our data as a sanity check\n","print('inputs_train shape: ',inputs_train.shape)\n","print('labels_train shape: ',labels_train.shape)\n","print('inputs_test shape: ',inputs_test.shape)\n","print('labels_test shape: ',labels_test.shape)\n","\n","# Use tf.data to batch and shuffle the dataset:\n","train_ds = tf.data.Dataset.from_tensor_slices((inputs_train, labels_train)).shuffle(10000).batch(32)\n","test_ds = tf.data.Dataset.from_tensor_slices((inputs_test, labels_test)).batch(32)\n","\n","# Example of a very simple Conv Net\n","class MyConvNet(tf.keras.Model):\n","  def __init__(self):\n","    # This function runs whenever you create an instance of your model.\n","    # Since it only runs once, you should initialise all your trainable variables here. Basically any layer that contains \"weights\" in your model.\n","    super(MyConvNet, self).__init__()\n","    self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n","    self.flatten = tf.keras.layers.Flatten()\n","    self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","    self.dense2 = tf.keras.layers.Dense(10)\n","\n","  def call(self, x):\n","    # This function runs every time you call the model: output = model(input).\n","    # Since it runs frequently, you should NOT initialise any trainable variables here.\n","    # If you do so, those weights will be re-initialised at every call, hence defeating the purpose of the training process.\n","    x = self.conv1(x)\n","    x = self.flatten(x)\n","    x = self.dense1(x)\n","    return self.dense2(x)\n","\n","# Create an instance of the model\n","model = MyConvNet()\n","\n","# We use SparseCategoricalCrossentropy as the loss function and Adam as the gradient descent function.\n","loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7)\n","\n","# We create the x_loss and x_accuracy objects to help keep track of model performance during training. \n","# The loss and accuracy values at each step of the training process are aggregated in the objects and can be printed out at the end of each training epoch.\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","\n","# The function train_step executes the forward and backward propagation of a single batch of images during the training process.\n","# As the TensorFlow documentation puts it, when you annotate a function with tf.function, you can still call it like any other function. \n","# But it will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.\n","@tf.function\n","def train_step(images, labels):\n","  with tf.GradientTape() as tape:\n","    # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n","    # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n","    predictions = model(images, training=True)\n","    train_step_loss = loss_function(labels, predictions)\n","  # Determine the gradients for each trainable variable (weights) based on the loss function\n","  gradients = tape.gradient(train_step_loss, model.trainable_variables)\n","  # Apply the optimiser to the gradients to perform gradient descent\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","  train_loss(train_step_loss)\n","  train_accuracy(labels, predictions)\n","\n","# The function test_step executes the inference (forward propagation) of a single batch of testing image.\n","def test_step(images, labels, confusion_matrix):\n","  # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n","  # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n","  predictions = model(images, training=False)\n","  test_step_loss = loss_function(labels, predictions)\n","  test_loss(test_step_loss)\n","  test_accuracy(labels, predictions)\n","  pred_class = np.argmax(predictions.numpy(), axis=-1)\n","  for i in range(labels.shape[0]):\n","    confusion_matrix[labels[i],pred_class[i]] += 1\n","\n","\n","max_epochs = 5\n","for epoch in range(max_epochs):\n","  # Reset the metrics at the start of the next epoch\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_loss.reset_states()\n","  test_accuracy.reset_states()\n","  confusion_matrix = np.zeros((10,10))\n","\n","  # Perform training across the entire train set\n","  for inputs, labels in train_ds:\n","    train_step(inputs, labels)\n","\n","  # Perform testing across the entire test set\n","  for test_inputs, test_labels in test_ds:\n","    test_step(test_inputs, test_labels, confusion_matrix)\n","\n","  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n","  print(template.format(epoch+1,\n","                        train_loss.result(),\n","                        train_accuracy.result()*100,\n","                        test_loss.result(),\n","                        test_accuracy.result()*100))\n","  print('Confusion matrix: rows represent labels, columns represent predictions')\n","  print(np.asarray(confusion_matrix,np.int32))"],"execution_count":0,"outputs":[]}]}